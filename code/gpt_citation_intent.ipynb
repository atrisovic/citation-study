{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "from os.path import basename\n",
    "from scipy.stats import gaussian_kde\n",
    "from utils import *\n",
    "from openai import OpenAI\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import List\n",
    "import instructor\n",
    "from enum import Enum\n",
    "import jsonlines\n",
    "\n",
    "client = instructor.from_openai(OpenAI(\n",
    "    api_key = \"sk-\"\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>mcllm</th>\n",
       "      <th>modelKey</th>\n",
       "      <th>paperId</th>\n",
       "      <th>modelId_x</th>\n",
       "      <th>mc</th>\n",
       "      <th>modelId_y</th>\n",
       "      <th>mc_reduced</th>\n",
       "      <th>urop_sentence</th>\n",
       "      <th>urop</th>\n",
       "      <th>urop_assignee</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>in recent years, many large-scale pre-trained ...</td>\n",
       "      <td>background</td>\n",
       "      <td>417_gpt-3_175b_(davinci)</td>\n",
       "      <td>d8d578d4ece329f17b025946587b1751721b9144</td>\n",
       "      <td>6b85b63579a916f705a8e10a49bd8d849d91b1fc</td>\n",
       "      <td>background</td>\n",
       "      <td>6b85b63579a916f705a8e10a49bd8d849d91b1fc</td>\n",
       "      <td>context</td>\n",
       "      <td>in recent years, many large-scale pre-trained ...</td>\n",
       "      <td>Background</td>\n",
       "      <td>Selinna</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>one of the known problems with contrastive tra...</td>\n",
       "      <td>background</td>\n",
       "      <td>1013_wave2vec_2.0_large</td>\n",
       "      <td>7f0c7c324675179f0e32c160d99c7066c7ab30ae</td>\n",
       "      <td>49a049dc85e2380dde80501a984878341dd8efdf</td>\n",
       "      <td>background</td>\n",
       "      <td>49a049dc85e2380dde80501a984878341dd8efdf</td>\n",
       "      <td>context</td>\n",
       "      <td>one of the known problems with contrastive tra...</td>\n",
       "      <td>Background</td>\n",
       "      <td>Selinna</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>further work in compound scaling yielded model...</td>\n",
       "      <td>background</td>\n",
       "      <td>377_efficientnet-l2</td>\n",
       "      <td>970cb7b5b25da0f1f8b000add10960680fe8cd2e</td>\n",
       "      <td>4f2eda8077dc7a69bb2b4e0a1a086cf054adb3f9</td>\n",
       "      <td>background</td>\n",
       "      <td>4f2eda8077dc7a69bb2b4e0a1a086cf054adb3f9</td>\n",
       "      <td>context</td>\n",
       "      <td>further work in compound scaling yielded model...</td>\n",
       "      <td>Future work</td>\n",
       "      <td>Selinna</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>due to the high computing cost, conducting a t...</td>\n",
       "      <td>background</td>\n",
       "      <td>417_gpt-3_175b_(davinci)</td>\n",
       "      <td>b6ec1e8f18185b4b3d46201359a440404575460c</td>\n",
       "      <td>6b85b63579a916f705a8e10a49bd8d849d91b1fc</td>\n",
       "      <td>background</td>\n",
       "      <td>6b85b63579a916f705a8e10a49bd8d849d91b1fc</td>\n",
       "      <td>context</td>\n",
       "      <td>due to the high computing cost, conducting a t...</td>\n",
       "      <td>Background</td>\n",
       "      <td>Selinna</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>##  1 introduction\\n\\n\\nthe multilingual bert ...</td>\n",
       "      <td>background</td>\n",
       "      <td>1064_bert-large</td>\n",
       "      <td>1234fcc1577a32b829d2886fdf68375b9d4525e9</td>\n",
       "      <td>df2b0e26d0599ce3e70df8a9da02e51594e0e992</td>\n",
       "      <td>background</td>\n",
       "      <td>df2b0e26d0599ce3e70df8a9da02e51594e0e992</td>\n",
       "      <td>context</td>\n",
       "      <td>##  1 introduction\\n\\nthe multilingual bert mo...</td>\n",
       "      <td>Background</td>\n",
       "      <td>Denis</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sentence       mcllm  \\\n",
       "0  in recent years, many large-scale pre-trained ...  background   \n",
       "1  one of the known problems with contrastive tra...  background   \n",
       "2  further work in compound scaling yielded model...  background   \n",
       "3  due to the high computing cost, conducting a t...  background   \n",
       "4  ##  1 introduction\\n\\n\\nthe multilingual bert ...  background   \n",
       "\n",
       "                   modelKey                                   paperId  \\\n",
       "0  417_gpt-3_175b_(davinci)  d8d578d4ece329f17b025946587b1751721b9144   \n",
       "1   1013_wave2vec_2.0_large  7f0c7c324675179f0e32c160d99c7066c7ab30ae   \n",
       "2       377_efficientnet-l2  970cb7b5b25da0f1f8b000add10960680fe8cd2e   \n",
       "3  417_gpt-3_175b_(davinci)  b6ec1e8f18185b4b3d46201359a440404575460c   \n",
       "4           1064_bert-large  1234fcc1577a32b829d2886fdf68375b9d4525e9   \n",
       "\n",
       "                                  modelId_x          mc  \\\n",
       "0  6b85b63579a916f705a8e10a49bd8d849d91b1fc  background   \n",
       "1  49a049dc85e2380dde80501a984878341dd8efdf  background   \n",
       "2  4f2eda8077dc7a69bb2b4e0a1a086cf054adb3f9  background   \n",
       "3  6b85b63579a916f705a8e10a49bd8d849d91b1fc  background   \n",
       "4  df2b0e26d0599ce3e70df8a9da02e51594e0e992  background   \n",
       "\n",
       "                                  modelId_y mc_reduced  \\\n",
       "0  6b85b63579a916f705a8e10a49bd8d849d91b1fc    context   \n",
       "1  49a049dc85e2380dde80501a984878341dd8efdf    context   \n",
       "2  4f2eda8077dc7a69bb2b4e0a1a086cf054adb3f9    context   \n",
       "3  6b85b63579a916f705a8e10a49bd8d849d91b1fc    context   \n",
       "4  df2b0e26d0599ce3e70df8a9da02e51594e0e992    context   \n",
       "\n",
       "                                       urop_sentence         urop  \\\n",
       "0  in recent years, many large-scale pre-trained ...   Background   \n",
       "1  one of the known problems with contrastive tra...   Background   \n",
       "2  further work in compound scaling yielded model...  Future work   \n",
       "3  due to the high computing cost, conducting a t...   Background   \n",
       "4  ##  1 introduction\\n\\nthe multilingual bert mo...   Background   \n",
       "\n",
       "  urop_assignee  \n",
       "0       Selinna  \n",
       "1       Selinna  \n",
       "2       Selinna  \n",
       "3       Selinna  \n",
       "4         Denis  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"data/task_9_citation_classification_mcllm - Sheet6.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CitationIntent(str, Enum):\n",
    "    background = 'background'\n",
    "    motivation = 'motivation'\n",
    "    uses = 'uses'\n",
    "    extends = 'extends'\n",
    "    differences = 'differences'\n",
    "    similarities = 'similarities'\n",
    "    future_work = 'future_work'\n",
    "    None\n",
    "\n",
    "class CitationSentence(BaseModel):\n",
    "    citation_intent: CitationIntent = Field(..., title=\"Citation intent in the citation sentence.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_msg = '''You are an assistant tasked with classifying citation sentences from scientific papers. The model is denoted with <cite></cite>. Each sentence should be categorized based on how a model is used in the reseach:\n",
    "\n",
    "Classify as 'background' if the sentence provides context or foundational information.\n",
    "Classify as 'motivation' if the sentence explains the reasons or needs for choosing or developing the model.\n",
    "Classify as 'extends' if the sentence indicates improvements or developments on the model.\n",
    "Classify as 'uses' if the sentence describes the application or operational use of the model.\n",
    "Classify as 'differences' if the sentence uses the model for difference comparisons or to highlight outcomes.\n",
    "Classify as 'similarities' if the sentence uses the model for similarity comparisons or to highlight outcomes.\n",
    "Classify as 'future_work' if the sentence discusses prospective studies or subsequent steps.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3it [00:02,  1.50it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "201it [01:55,  1.74it/s]\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "for index, row in tqdm(df.iterrows()):\n",
    "    sentence = row['sentence']\n",
    "    \n",
    "    user_msg = (\n",
    "        \"Extract the citation intent from the sentence: \"\n",
    "        + sentence\n",
    "    )\n",
    "    if sentence is None:\n",
    "        continue\n",
    "    \n",
    "    chat_completion = client.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        response_model=CitationSentence,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_msg},\n",
    "            {\"role\": \"user\", \"content\": user_msg},\n",
    "        ],\n",
    "    )\n",
    "    results.append(chat_completion.citation_intent.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['gpt'] = results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>mcllm</th>\n",
       "      <th>modelKey</th>\n",
       "      <th>paperId</th>\n",
       "      <th>modelId_x</th>\n",
       "      <th>mc</th>\n",
       "      <th>modelId_y</th>\n",
       "      <th>mc_reduced</th>\n",
       "      <th>urop_sentence</th>\n",
       "      <th>urop</th>\n",
       "      <th>urop_assignee</th>\n",
       "      <th>gpt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>in recent years, many large-scale pre-trained ...</td>\n",
       "      <td>background</td>\n",
       "      <td>417_gpt-3_175b_(davinci)</td>\n",
       "      <td>d8d578d4ece329f17b025946587b1751721b9144</td>\n",
       "      <td>6b85b63579a916f705a8e10a49bd8d849d91b1fc</td>\n",
       "      <td>background</td>\n",
       "      <td>6b85b63579a916f705a8e10a49bd8d849d91b1fc</td>\n",
       "      <td>context</td>\n",
       "      <td>in recent years, many large-scale pre-trained ...</td>\n",
       "      <td>Background</td>\n",
       "      <td>Selinna</td>\n",
       "      <td>background</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>one of the known problems with contrastive tra...</td>\n",
       "      <td>background</td>\n",
       "      <td>1013_wave2vec_2.0_large</td>\n",
       "      <td>7f0c7c324675179f0e32c160d99c7066c7ab30ae</td>\n",
       "      <td>49a049dc85e2380dde80501a984878341dd8efdf</td>\n",
       "      <td>background</td>\n",
       "      <td>49a049dc85e2380dde80501a984878341dd8efdf</td>\n",
       "      <td>context</td>\n",
       "      <td>one of the known problems with contrastive tra...</td>\n",
       "      <td>Background</td>\n",
       "      <td>Selinna</td>\n",
       "      <td>motivation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>further work in compound scaling yielded model...</td>\n",
       "      <td>background</td>\n",
       "      <td>377_efficientnet-l2</td>\n",
       "      <td>970cb7b5b25da0f1f8b000add10960680fe8cd2e</td>\n",
       "      <td>4f2eda8077dc7a69bb2b4e0a1a086cf054adb3f9</td>\n",
       "      <td>background</td>\n",
       "      <td>4f2eda8077dc7a69bb2b4e0a1a086cf054adb3f9</td>\n",
       "      <td>context</td>\n",
       "      <td>further work in compound scaling yielded model...</td>\n",
       "      <td>Future work</td>\n",
       "      <td>Selinna</td>\n",
       "      <td>future_work</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>due to the high computing cost, conducting a t...</td>\n",
       "      <td>background</td>\n",
       "      <td>417_gpt-3_175b_(davinci)</td>\n",
       "      <td>b6ec1e8f18185b4b3d46201359a440404575460c</td>\n",
       "      <td>6b85b63579a916f705a8e10a49bd8d849d91b1fc</td>\n",
       "      <td>background</td>\n",
       "      <td>6b85b63579a916f705a8e10a49bd8d849d91b1fc</td>\n",
       "      <td>context</td>\n",
       "      <td>due to the high computing cost, conducting a t...</td>\n",
       "      <td>Background</td>\n",
       "      <td>Selinna</td>\n",
       "      <td>motivation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>##  1 introduction\\n\\n\\nthe multilingual bert ...</td>\n",
       "      <td>background</td>\n",
       "      <td>1064_bert-large</td>\n",
       "      <td>1234fcc1577a32b829d2886fdf68375b9d4525e9</td>\n",
       "      <td>df2b0e26d0599ce3e70df8a9da02e51594e0e992</td>\n",
       "      <td>background</td>\n",
       "      <td>df2b0e26d0599ce3e70df8a9da02e51594e0e992</td>\n",
       "      <td>context</td>\n",
       "      <td>##  1 introduction\\n\\nthe multilingual bert mo...</td>\n",
       "      <td>Background</td>\n",
       "      <td>Denis</td>\n",
       "      <td>background</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sentence       mcllm  \\\n",
       "0  in recent years, many large-scale pre-trained ...  background   \n",
       "1  one of the known problems with contrastive tra...  background   \n",
       "2  further work in compound scaling yielded model...  background   \n",
       "3  due to the high computing cost, conducting a t...  background   \n",
       "4  ##  1 introduction\\n\\n\\nthe multilingual bert ...  background   \n",
       "\n",
       "                   modelKey                                   paperId  \\\n",
       "0  417_gpt-3_175b_(davinci)  d8d578d4ece329f17b025946587b1751721b9144   \n",
       "1   1013_wave2vec_2.0_large  7f0c7c324675179f0e32c160d99c7066c7ab30ae   \n",
       "2       377_efficientnet-l2  970cb7b5b25da0f1f8b000add10960680fe8cd2e   \n",
       "3  417_gpt-3_175b_(davinci)  b6ec1e8f18185b4b3d46201359a440404575460c   \n",
       "4           1064_bert-large  1234fcc1577a32b829d2886fdf68375b9d4525e9   \n",
       "\n",
       "                                  modelId_x          mc  \\\n",
       "0  6b85b63579a916f705a8e10a49bd8d849d91b1fc  background   \n",
       "1  49a049dc85e2380dde80501a984878341dd8efdf  background   \n",
       "2  4f2eda8077dc7a69bb2b4e0a1a086cf054adb3f9  background   \n",
       "3  6b85b63579a916f705a8e10a49bd8d849d91b1fc  background   \n",
       "4  df2b0e26d0599ce3e70df8a9da02e51594e0e992  background   \n",
       "\n",
       "                                  modelId_y mc_reduced  \\\n",
       "0  6b85b63579a916f705a8e10a49bd8d849d91b1fc    context   \n",
       "1  49a049dc85e2380dde80501a984878341dd8efdf    context   \n",
       "2  4f2eda8077dc7a69bb2b4e0a1a086cf054adb3f9    context   \n",
       "3  6b85b63579a916f705a8e10a49bd8d849d91b1fc    context   \n",
       "4  df2b0e26d0599ce3e70df8a9da02e51594e0e992    context   \n",
       "\n",
       "                                       urop_sentence         urop  \\\n",
       "0  in recent years, many large-scale pre-trained ...   Background   \n",
       "1  one of the known problems with contrastive tra...   Background   \n",
       "2  further work in compound scaling yielded model...  Future work   \n",
       "3  due to the high computing cost, conducting a t...   Background   \n",
       "4  ##  1 introduction\\n\\nthe multilingual bert mo...   Background   \n",
       "\n",
       "  urop_assignee          gpt  \n",
       "0       Selinna   background  \n",
       "1       Selinna   motivation  \n",
       "2       Selinna  future_work  \n",
       "3       Selinna   motivation  \n",
       "4         Denis   background  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"data/citation_intent_mcll_mc_urop_gpt_200.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
